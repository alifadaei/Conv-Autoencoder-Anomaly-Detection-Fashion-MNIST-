{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Convolutional Autoencoder Anomaly Detection (Fashion-MNIST)\n\n**Goal:** Unsupervised anomaly detection using **reconstruction error**.  \nWe train a convolutional autoencoder (CAE) on one **normal** class and treat other classes as anomalies.\n\n**Pipeline**\n1) Train CAE on normal-class images only  \n2) Score test images by reconstruction error  \n3) Evaluate with ROC-AUC / PR-AUC and a thresholded confusion matrix  \n4) Visualize reconstructions and top anomalies\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Install & imports\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# (Optional for Colab)\n# !pip -q install torch torchvision numpy matplotlib tqdm scikit-learn\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, random, time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\n\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc, confusion_matrix\n\nprint(\"torch:\", torch.__version__)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"device:\", device)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Config\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "SEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# Fashion-MNIST labels:\n# 0: T-shirt/top, 1: Trouser, 2: Pullover, 3: Dress, 4: Coat,\n# 5: Sandal, 6: Shirt, 7: Sneaker, 8: Bag, 9: Ankle boot\nNORMAL_CLASS = 0\n\nEPOCHS = 10\nBATCH_SIZE = 256\nLR = 1e-3\nWEIGHT_DECAY = 1e-5\n\nOUT_DIR = \"outputs_cae_anomaly\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nprint(\"NORMAL_CLASS =\", NORMAL_CLASS)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Data\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tf = transforms.Compose([transforms.ToTensor()])\n\ntrain_ds = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=tf)\ntest_ds  = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=tf)\n\ntrain_targets = np.array(train_ds.targets)\nnormal_train_idx = np.where(train_targets == NORMAL_CLASS)[0]\ntrain_normal = Subset(train_ds, normal_train_idx)\n\ntest_targets = np.array(test_ds.targets)\ny_true = (test_targets != NORMAL_CLASS).astype(np.int64)  # 1=anomaly, 0=normal\n\ntrain_dl = DataLoader(train_normal, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=(device==\"cuda\"))\ntest_dl  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=(device==\"cuda\"))\n\nprint(\"Train (normal only):\", len(train_normal))\nprint(\"Test:\", len(test_ds), \"| anomaly ratio:\", y_true.mean())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Model (Convolutional Autoencoder)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class ConvAutoencoder(nn.Module):\n    def __init__(self, latent_dim=64):\n        super().__init__()\n        self.enc = nn.Sequential(\n            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # 16x14x14\n            nn.ReLU(inplace=True),\n            nn.Conv2d(16, 32, 3, stride=2, padding=1), # 32x7x7\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, stride=1, padding=1), # 64x7x7\n            nn.ReLU(inplace=True),\n        )\n        self.enc_fc = nn.Linear(64*7*7, latent_dim)\n\n        self.dec_fc = nn.Linear(latent_dim, 64*7*7)\n        self.dec = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), # 32x14x14\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1), # 16x28x28\n            nn.ReLU(inplace=True),\n            nn.Conv2d(16, 1, 3, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def encode(self, x):\n        h = self.enc(x).view(x.size(0), -1)\n        return self.enc_fc(h)\n\n    def decode(self, z):\n        h = self.dec_fc(z).view(z.size(0), 64, 7, 7)\n        return self.dec(h)\n\n    def forward(self, x):\n        z = self.encode(x)\n        return self.decode(z)\n\nmodel = ConvAutoencoder(latent_dim=64).to(device)\nprint(model)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Train on normal data only\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "criterion = nn.MSELoss()\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\ntrain_losses = []\nmodel.train()\n\nstart = time.time()\nfor epoch in range(1, EPOCHS+1):\n    running = 0.0\n    pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{EPOCHS}\")\n    for x, _ in pbar:\n        x = x.to(device, non_blocking=True)\n        xhat = model(x)\n        loss = criterion(xhat, x)\n\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        running += loss.item()\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n    avg = running / len(train_dl)\n    train_losses.append(avg)\n    print(f\"Epoch {epoch}: mse={avg:.6f}\")\n\nprint(f\"Training time: {(time.time()-start)/60:.2f} min\")\ntorch.save({\"model\": model.state_dict(), \"config\": {\"NORMAL_CLASS\": NORMAL_CLASS}}, os.path.join(OUT_DIR, \"cae.pt\"))\nprint(\"Saved:\", os.path.join(OUT_DIR, \"cae.pt\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Plot training loss\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.plot(range(1, len(train_losses)+1), train_losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE\")\nplt.title(\"CAE Training Loss (normal-only)\")\nplt.grid(True)\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Score test images by reconstruction error\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@torch.no_grad()\ndef reconstruction_errors(model, dataloader):\n    model.eval()\n    errs = []\n    for x, _ in tqdm(dataloader, desc=\"Scoring\"):\n        x = x.to(device, non_blocking=True)\n        xhat = model(x)\n        e = ((xhat - x) ** 2).view(x.size(0), -1).mean(dim=1).cpu().numpy()\n        errs.append(e)\n    return np.concatenate(errs, axis=0)\n\nscores = reconstruction_errors(model, test_dl)  # higher => more anomalous\nprint(\"scores:\", scores.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Score distribution (normal vs anomaly)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.hist(scores[y_true==0], bins=50, alpha=0.7, label=\"Normal\")\nplt.hist(scores[y_true==1], bins=50, alpha=0.7, label=\"Anomaly\")\nplt.xlabel(\"Reconstruction error (MSE)\")\nplt.ylabel(\"Count\")\nplt.title(\"Reconstruction Error Distribution\")\nplt.grid(True)\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Metrics: ROC-AUC, PR-AUC, and thresholding\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "roc_auc = roc_auc_score(y_true, scores)\nfpr, tpr, thr = roc_curve(y_true, scores)\n\nprec, rec, _ = precision_recall_curve(y_true, scores)\npr_auc = auc(rec, prec)\n\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\nprint(f\"PR-AUC : {pr_auc:.4f}\")\n\nplt.figure(figsize=(5,5))\nplt.plot(fpr, tpr)\nplt.plot([0,1],[0,1], linestyle=\"--\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(f\"ROC (AUC={roc_auc:.3f})\")\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.plot(rec, prec)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(f\"Precision-Recall (AUC={pr_auc:.3f})\")\nplt.grid(True)\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Choose threshold via Youdenâ€™s J and compute confusion matrix\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "j = tpr - fpr\nbest_i = j.argmax()\nbest_thr = thr[best_i]\nprint(\"Best threshold:\", best_thr)\n\ny_pred = (scores >= best_thr).astype(np.int64)\ncm = confusion_matrix(y_true, y_pred)\ntn, fp, fn, tp = cm.ravel()\n\nprint(\"Confusion matrix:\\n\", cm)\nprint(f\"TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\nprint(\"Precision:\", tp / max(tp+fp, 1))\nprint(\"Recall   :\", tp / max(tp+fn, 1))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Visualize reconstructions and top anomalies\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@torch.no_grad()\ndef recon_pair(model, dataset, idx):\n    model.eval()\n    x, y = dataset[idx]\n    x_in = x.unsqueeze(0).to(device)\n    xhat = model(x_in).squeeze(0).cpu()\n    return x, xhat, int(y)\n\ndef show_recon_grid(indices, title):\n    plt.figure(figsize=(12,4))\n    for i, idx in enumerate(indices):\n        x, xhat, y = recon_pair(model, test_ds, idx)\n        plt.subplot(2, len(indices), i+1)\n        plt.imshow(x.squeeze(0), cmap=\"gray\")\n        plt.axis(\"off\")\n        plt.title(f\"y={y}\")\n        plt.subplot(2, len(indices), len(indices)+i+1)\n        plt.imshow(xhat.squeeze(0), cmap=\"gray\")\n        plt.axis(\"off\")\n        plt.title(\"recon\")\n    plt.suptitle(title)\n    plt.show()\n\n# random normals and anomalies\ntest_targets = np.array(test_ds.targets)\nnormal_idx = np.where(test_targets == NORMAL_CLASS)[0]\nanom_idx = np.where(test_targets != NORMAL_CLASS)[0]\nnp.random.shuffle(normal_idx)\nnp.random.shuffle(anom_idx)\n\nshow_recon_grid(normal_idx[:6], \"Normal samples (top) vs reconstructions (bottom)\")\nshow_recon_grid(anom_idx[:6], \"Anomaly samples (top) vs reconstructions (bottom)\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Top-k most anomalous samples (highest reconstruction error)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "top_k = 12\ntop_idx = np.argsort(-scores)[:top_k]\nplt.figure(figsize=(14,4))\nfor i, idx in enumerate(top_idx):\n    x, xhat, y = recon_pair(model, test_ds, int(idx))\n    plt.subplot(2, top_k, i+1)\n    plt.imshow(x.squeeze(0), cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(f\"y={y}\")\n    plt.subplot(2, top_k, top_k+i+1)\n    plt.imshow(xhat.squeeze(0), cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(f\"e={scores[idx]:.4f}\")\nplt.suptitle(\"Top anomalies (highest reconstruction error)\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Next steps (to make it even stronger)\n- Try different NORMAL_CLASS values and compare ROC-AUC  \n- Use a **denoising autoencoder** (add noise, reconstruct clean)  \n- Replace CAE with a **VAE** and use negative log-likelihood  \n- Add latent-space visualization (PCA/t-SNE on `z`)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}